{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d4007a-acfc-4461-aa2d-cfff86fbeca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Using cached albumentations-1.4.8-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.11.4)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.23.2)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from albumentations) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (4.11.0)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from albumentations) (1.4.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from albumentations) (2.7.1)\n",
      "Requirement already satisfied: albucore>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from albumentations) (0.0.6)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /opt/conda/lib/python3.10/site-packages (from albumentations) (4.9.0.80)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.10/site-packages (from albucore>=0.0.4->albumentations) (2.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.7.0->albumentations) (2.18.2)\n",
      "Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (10.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (2024.4.24)\n",
      "Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (24.0)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.3.2->albumentations) (3.5.0)\n",
      "Using cached albumentations-1.4.8-py3-none-any.whl (156 kB)\n",
      "Installing collected packages: albumentations\n",
      "Successfully installed albumentations-1.4.8\n"
     ]
    }
   ],
   "source": [
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057bb6d2-b3e8-49d3-b9fd-1755468f6718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor\n",
    ")\n",
    "from PIL import Image\n",
    "from CustomDataset import CustomDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f255bec9-b1ba-4413-8b96-4cc923d0c50c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = \"/home/jupyter/novice\"\n",
    "jsonl_path = os.path.join(base_dir, \"vlm.jsonl\")\n",
    "images_dir = os.path.join(base_dir, \"images\")\n",
    "cropped_images_dir = \"/home/jupyter/til-24-base/derrick/clip/images\"\n",
    "os.makedirs(cropped_images_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3d84a3-3ac7-492e-a226-536e8fc9fed2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The projection layer and logit scale weights `['visual_projection.weight', 'text_projection.weight', 'logit_scale']` are newly initialized. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = VisionTextDualEncoderModel.from_vision_text_pretrained(\"openai/clip-vit-base-patch16\", \"roberta-base\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e658eaa-31c5-4438-874d-068692cc06cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "image_paths, captions = crop_and_save_images(jsonl_path, images_dir)\n",
    "\n",
    "# Split the dataset into 90% train and 10% test\n",
    "train_image_paths, val_image_paths, train_captions, val_captions = train_test_split(\n",
    "    image_paths, captions, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e56282b8-3a02-4c5d-a33e-fd3ba48151b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n"
     ]
    }
   ],
   "source": [
    "print(config.vision_config.image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605d2e2a-badf-4c8c-b0d4-916accdd764b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224, [0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711]\n"
     ]
    }
   ],
   "source": [
    "image_size = config.vision_config.image_size\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "\n",
    "print(f\"{image_size}, {mean}, {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "998ab4cc-cd7e-4f47-8e02-6d61d649e9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_size = config.vision_config.image_size\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "\n",
    "transform = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=image_size, interpolation=cv2.INTER_CUBIC, p=1.0),\n",
    "        A.CenterCrop(height=image_size, width=image_size, p=1.0),\n",
    "        A.Rotate(limit=(-10, 10), p=0.2),\n",
    "        A.Blur(blur_limit=(3, 3), p=0.2),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64940509-04e0-46ba-a644-a9e8c84800f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose([\n",
      "  SmallestMaxSize(always_apply=False, p=1.0, max_size=[224], interpolation=2),\n",
      "  CenterCrop(always_apply=False, p=1.0, height=224, width=224),\n",
      "  Rotate(always_apply=False, p=0.2, limit=(-10, 10), interpolation=1, border_mode=4, value=None, mask_value=None, rotate_method='largest_box', crop_border=False),\n",
      "  Blur(always_apply=False, p=0.2, blur_limit=(3, 3)),\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711], max_pixel_value=255.0, normalization='standard'),\n",
      "  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)\n"
     ]
    }
   ],
   "source": [
    "print(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68d17b13-6942-420f-8d4b-ec4c652ec545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CustomDataset(image_paths=train_image_paths, captions=train_captions, tokenizer=tokenizer, transform=transform)\n",
    "val_dataset = CustomDataset(image_paths=val_image_paths, captions=val_captions, tokenizer=tokenizer, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ed411ba-8af1-4a61-a455-191ca184e62e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir=\"clip-augment-finetune\"\n",
    "learning_rate=1e-5\n",
    "weight_decay=0.1\n",
    "batch_size=10\n",
    "num_epochs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92398e36-848c-46a0-a9f0-a56807f7b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    images = torch.stack([image for image, target in examples])\n",
    "    input_ids = torch.tensor([target[\"input_ids\"] for image, target in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([target[\"attention_mask\"] for image, target in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": images,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17539e85-08af-432d-9a6d-62a7ee1eaede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9433' max='20190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9433/20190 3:35:49 < 4:06:09, 0.73 it/s, Epoch 7.01/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.550600</td>\n",
       "      <td>0.747710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.386600</td>\n",
       "      <td>0.234000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.153100</td>\n",
       "      <td>0.116912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.097536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.074300</td>\n",
       "      <td>0.086864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.075488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.062800</td>\n",
       "      <td>0.078358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "trainer.save_model(os.path.join(output_dir, \"saved_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(output_dir, \"saved_model\"))\n",
    "image_processor.save_pretrained(os.path.join(output_dir, \"saved_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e657f1-4b50-42ff-927a-0c14fac9947a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
