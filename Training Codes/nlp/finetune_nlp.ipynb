{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7af259ab-a1de-40b1-a616-c75960784b12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Load the JSONL file\n",
    "file_path = \"./nlp.jsonl\"\n",
    "additional_file_path = \"./output.jsonl\"\n",
    "data = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "        \n",
    "with open(additional_file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(len(data))\n",
    "random.shuffle(data)\n",
    "# Prepare the dataset\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09fff12f-395b-48f9-a623-2c571bd8d44c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['key', 'transcript', 'tool', 'heading', 'target', 'tokens', 'labels'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Define the label mapping\n",
    "label_list = ['O', 'B-TOOL', 'B-HEADING', 'B-TARGET']\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "num_to_text = {\n",
    "    \"0\": \"zero\",\n",
    "    \"1\": \"one\",\n",
    "    \"2\": \"two\",\n",
    "    \"3\": \"three\",\n",
    "    \"4\": \"four\",\n",
    "    \"5\": \"five\",\n",
    "    \"6\": \"six\",\n",
    "    \"7\": \"seven\",\n",
    "    \"8\": \"eight\",\n",
    "    \"9\": \"niner\"\n",
    "}\n",
    "\n",
    "# Helper function to find the index of a sublist in a list\n",
    "def find_sublist(lst, sublist):\n",
    "    for i in range(len(lst) - len(sublist) + 1):\n",
    "        if lst[i:i+len(sublist)] == sublist:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "\n",
    "def convert_num_to_text(digits):\n",
    "    digits = str(digits)\n",
    "    words = [num_to_text[digit] for digit in digits]\n",
    "    return words\n",
    "    \n",
    "# Helper function to create labels\n",
    "def create_labels(transcript, tool, heading, target):\n",
    "    tokens = re.split(r'(\\W+)', transcript)\n",
    "    tokens = [word for word in tokens if word.strip()]\n",
    "    labels = ['O'] * len(tokens)\n",
    "    \n",
    "    #print(tokens)\n",
    "    \n",
    "    # Process heading\n",
    "    heading_tokens = convert_num_to_text(heading)\n",
    "    #print(heading_tokens)\n",
    "    start_idx = find_sublist(tokens, heading_tokens)\n",
    "    if start_idx != -1:\n",
    "        for i in range(start_idx, start_idx + len(heading_tokens)):\n",
    "            labels[i] = 'B-HEADING'\n",
    "    \n",
    "    # Process tool\n",
    "    tool_tokens = re.split(r'(\\W+)', tool)\n",
    "    tool_tokens = [word for word in tool_tokens if word.strip()]\n",
    "    start_idx = find_sublist(tokens, tool_tokens)\n",
    "    if start_idx != -1:\n",
    "        for i in range(start_idx, start_idx + len(tool_tokens)):\n",
    "            labels[i] = 'B-TOOL'\n",
    "    \n",
    "    # Process target\n",
    "    target_tokens = re.split(r'(\\W+)', target)\n",
    "    target_tokens = [word for word in target_tokens if word.strip()]\n",
    "    start_idx = find_sublist(tokens, target_tokens)\n",
    "    if start_idx != -1:\n",
    "        for i in range(start_idx, start_idx + len(target_tokens)):\n",
    "            labels[i] = 'B-TARGET'\n",
    "    \n",
    "    #print(labels)\n",
    "    \n",
    "    return tokens, [label_map[label] for label in labels]\n",
    "\n",
    "\n",
    "# Apply the function to the DataFrame\n",
    "results = df.apply(lambda row: create_labels(row['transcript'], row['tool'], row['heading'], row['target']), axis=1)\n",
    "\n",
    "# Create new columns from the results\n",
    "df['tokens'] = results.apply(lambda x: x[0])\n",
    "df['labels'] = results.apply(lambda x: x[1])\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(df['tokens'][0])\n",
    "#print(df['labels'][0])\n",
    "# Convert to Hugging Face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "train_dataset = dataset.select(range(int(0.8 * len(dataset))))\n",
    "test_dataset = dataset.select(range(int(0.8 * len(dataset)), len(dataset)))\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5106d3c6-a9a6-4d4d-899a-e93eb1cdea78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['key', 'transcript', 'tool', 'heading', 'target', 'tokens', 'labels'],\n",
       "    num_rows: 24000\n",
       "})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "04878177-5518-4dfa-a004-3feaa4a65c59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Set up the device\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5f429da5-c7ac-4888-b336-eec24cc5ebd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8218ee6ee5354865b3ab0efd2b1ca9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b26f650efa94b60ad67c287e6db7a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tokens'], truncation=True, is_split_into_words=True, padding=\"max_length\", max_length=25)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_tokenized_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "test_tokenized_dataset = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92d95990-87d6-4e72-b521-3e2ddad15426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 14785, 'transcript': 'target is tiny, yellow and white glider. tool to deploy is patriot missile system. heading is two niner five', 'tool': 'patriot missile system', 'heading': '295', 'target': 'tiny, yellow and white glider', 'tokens': ['target', 'is', 'tiny', ', ', 'yellow', 'and', 'white', 'glider', '. ', 'tool', 'to', 'deploy', 'is', 'patriot', 'missile', 'system', '. ', 'heading', 'is', 'two', 'niner', 'five'], 'labels': [-100, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 2, 2, -100, 2, -100], 'input_ids': [101, 4539, 2003, 4714, 1010, 3756, 1998, 2317, 18788, 1012, 6994, 2000, 21296, 2003, 16419, 7421, 2291, 1012, 5825, 2003, 2048, 3157, 2099, 2274, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(train_tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "48468fad-e999-4b62-929d-bb3d0ec7b223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c4959f34-3e22-4257-8288-591615668d7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 33:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.003759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.001697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.002584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.001330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.001784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.001961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.002127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.004722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.002832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.003252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training arguments with logging enabled\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    save_total_limit=1,  # Only keep the best model\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=test_tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(\"./fine-tuned-model30000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "498275b8-3e69-40e7-8f2d-1e5605986eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  4539,  2003,  2304,  2417,  1998,  2829,  7739,  6994,  2000,\n",
      "         21296,  2003, 20248, 20100,  5649, 14549,  5825,  2003,  2698,  2028,\n",
      "          5717,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "[('target', 'O'), ('is', 'O'), ('black', 'B-TARGET'), ('red', 'B-TARGET'), ('and', 'B-TARGET'), ('brown', 'B-TARGET'), ('helicopter', 'B-TARGET'), ('tool', 'O'), ('to', 'O'), ('deploy', 'O'), ('is', 'O'), ('geostationary', 'B-TOOL'), ('satellites', 'B-TOOL'), ('heading', 'O'), ('is', 'O'), ('seven', 'B-HEADING'), ('one', 'B-HEADING'), ('zero', 'B-HEADING')]\n",
      "{'tool': 'geostationary satellites', 'heading': '710', 'target': 'black red and brown helicopter'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_name = \"./fine-tuned-model30000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define label mapping\n",
    "id2label = {0: 'O', 1: 'B-TOOL', 2: 'B-HEADING', 3: 'B-TARGET'}\n",
    "\n",
    "# Mapping text numbers to their digit equivalents\n",
    "text_to_num = {\n",
    "    \"zero\": \"0\",\n",
    "    \"one\": \"1\",\n",
    "    \"two\": \"2\",\n",
    "    \"three\": \"3\",\n",
    "    \"four\": \"4\",\n",
    "    \"five\": \"5\",\n",
    "    \"six\": \"6\",\n",
    "    \"seven\": \"7\",\n",
    "    \"eight\": \"8\",\n",
    "    \"niner\": \"9\"\n",
    "}\n",
    "\n",
    "def convert_heading_to_number(heading_tokens):\n",
    "    # Strip punctuation and convert text to numbers\n",
    "    output = \"\".join([text_to_num.get(token.rstrip('.,'), token) for token in heading_tokens])\n",
    "    \n",
    "    while len(output) < 3:\n",
    "        output = \"0\" + output\n",
    "    \n",
    "    return output\n",
    "\n",
    "def predict(transcript):\n",
    "    # Tokenize the input transcript\n",
    "    tokens = transcript.replace('.', '').split()\n",
    "    \n",
    "    tokenized_inputs = tokenizer(tokens, is_split_into_words=True, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    print(tokenized_inputs)\n",
    "    \n",
    "    inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in tokenized_inputs.items()}\n",
    "    \n",
    "    # Get the model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    \n",
    "    # Convert the predictions to labels\n",
    "    predictions = torch.argmax(outputs, dim=2).cpu().numpy()[0]\n",
    "\n",
    "    # Align the labels with the original tokens\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx, prediction in zip(word_ids, predictions):\n",
    "        if word_idx is None or word_idx == previous_word_idx:\n",
    "            continue\n",
    "        aligned_labels.append((tokens[word_idx], id2label[prediction]))\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    # Convert the aligned labels to the desired output format\n",
    "    tool, heading, target = [], [], []\n",
    "    print(aligned_labels)\n",
    "    for token, label in aligned_labels:\n",
    "        if label == 'B-TOOL':\n",
    "            tool.append(token)\n",
    "        elif label == 'B-HEADING':\n",
    "            heading.append(token)\n",
    "        elif label == 'B-TARGET':\n",
    "            target.append(token)\n",
    "    \n",
    "    output = {\n",
    "        \"tool\": \" \".join(tool).strip(',.'),\n",
    "        \"heading\": convert_heading_to_number(heading),\n",
    "        \"target\": \" \".join(target).strip(',.')\n",
    "    }\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "example_transcript = \"target is black red and brown helicopter tool to deploy is geostationary satellites. heading is seven one zero\"\n",
    "print(predict(example_transcript))\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
